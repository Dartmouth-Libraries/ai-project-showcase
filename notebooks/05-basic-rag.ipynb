{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Retrieval-Augmented Generation\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a young and constantly evolving field. It is based on the assumptions that so-called hallucinations (factually incorrect statements by the LLM) can be avoided if relevant context is provided with a user's question.\n",
    "\n",
    "In a nutshell, RAG attempts to automate the step of providing relevant context. In its most common implementation, RAG involves a large collection of potentially relevant documents (the _knowledge base_), which are then indexed and accessed based on the user's message using some sort of retrieval mechanism.\n",
    "\n",
    "In this example, we will deal with a simpler version: We expect the user to specify the knowledge base in the form of a single website, or a YouTube video transcript. We will then automate the step of retrieving this context and add it to any question the user might have in its entirety (a.k.a. _context stuffing_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since RAG is a new concept, an LLM may give a helpful answer if asked without any context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"RAG can have multiple meanings depending on the context. \\n\\n1. **Roller Coaster**: RAG is an abbreviation for Roller Coaster. \\n2. **Reactive Attachment Disorder**: RAG is an abbreviation for Reactive Attachment Disorder, a mental health disorder in children that involves difficulty forming emotional connections with others.\\n3. **Reagent**: In chemistry, RAG can refer to a reagent, a substance used in chemical reactions.\\n4. **Radio Amateur Guild**: RAG is an abbreviation for Radio Amateur Guild, an amateur radio club. \\n5. **RAG**: In some gaming communities, RAG is an abbreviation for Rolled Against God, which is typically used as a humorous expression in the context of tabletop role-playing games.\\n\\nWithout more context, it's difficult to determine which definition is most relevant.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 40, 'total_tokens': 206}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'system_fingerprint': '2.2.0-sha-db7e043', 'finish_reason': 'eos_token', 'logprobs': None}, id='run-1f01abd1-bb97-4d52-9392-45d11975949a-0', usage_metadata={'input_tokens': 40, 'output_tokens': 166, 'total_tokens': 206})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"What is RAG?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know [a potentially helpful website](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), we can leverage the LangChain framework to stuff its contents into our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nRetrieval-augmented generation - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\n Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nProcess\\n\\n\\n\\n\\nToggle Process subsection\\n\\n\\n\\n\\n\\n1.1\\nIndexing\\n\\n\\n\\n\\n\\n\\n\\n\\n1.2\\nRetrieval\\n\\n\\n\\n\\n\\n\\n\\n\\n1.3\\nAugmentation\\n\\n\\n\\n\\n\\n\\n\\n\\n1.4\\nGeneration\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nImprovements\\n\\n\\n\\n\\nToggle Improvements subsection\\n\\n\\n\\n\\n\\n2.1\\nEncoder\\n\\n\\n\\n\\n\\n\\n\\n\\n2.2\\nRetriever-centric methods\\n\\n\\n\\n\\n\\n\\n\\n\\n2.3\\nLanguage model\\n\\n\\n\\n\\n\\n\\n\\n\\n2.4\\nChunking\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nChallenges\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nRetrieval-augmented generation\\n\\n\\n\\n2 languages\\n\\n\\n\\n\\nDeutsch한국어\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nType of information retrieval using LLMs\\nRetrieval augmented generation (RAG) is a type of generative artificial intelligence that has information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information in preference to information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.[1]  \\nUse cases include providing chatbot access to internal company data, or giving factual information only from an authoritative source.[2]\\n\\n\\nProcess[edit]\\nThe RAG process is made up of four key stages. First, all the data must be prepared and indexed for use by the LLM. Thereafter, each query consists of a retrieval, augmentation and a generation phase.[1]\\n\\nIndexing[edit]\\nThe data to be referenced must first be converted into LLM embeddings, numerical representations in the form of large vectors. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs).[1] These embeddings are then stored in a vector database to allow for document retrieval.\\n\\nOverview of RAG process, combining external documents and user input into an LLM prompt to get tailored output\\nRetrieval[edit]\\nGiven a user query, a document retriever is first called to select the most relevant documents which will be used to augment the query.[3] This comparison can be done using a variety of methods, which depend in part on the type of indexing used.[1]\\n\\nAugmentation[edit]\\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user\\'s original query.[2] Newer implementations (as of 2023[update]) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains, and using memory and self-improvement to learn from previous retrievals.[1]\\n\\nGeneration[edit]\\nFinally, the LLM can generate output based on both the query and the retrieved documents.[4] Some models incorporate extra steps to improve output such as the re-ranking of retrieved information, context selection and fine tuning.[1]\\n\\nImprovements[edit]\\nImprovements to the basic process above can be applied at different stages in the RAG flow. \\n\\nEncoder[edit]\\nThese methods center around the encoding of text as either dense or sparse vectors. Sparse vectors, used to encode the identity of a word, are typically dictionary length[clarification needed] and contain almost all zeros. Dense vectors, used to encode meaning, are much smaller and contain far fewer zeros. Several enhancements can be made in the way similarities are calculated in the vector stores (databases).  \\n\\nPerformance can be improved with faster dot products, approximate nearest neighors, or centroid searches.[5]\\nAccuracy can be improved with Late Interactions.[clarification needed][6]\\nHybrid vectors: dense vector representations can be combined with sparse one-hot vectors in order to use the faster sparse dot products rather than the slower dense ones.[7]  Other[clarification needed] methods can combine sparse methods (BM25, SPLADE) with dense ones like DRAGON.\\nRetriever-centric methods[edit]\\nThese methods focus on improving the quality of hits from the vector database:\\n\\n\\n pre-train the retriever using the Inverse Cloze Task.[8] \\n progressive data augmentation.  The method of Dragon samples difficult negatives to train a dense vector  retriever.[9] \\n Under supervision, train the retriever for a given generator.  Given a prompt and the desired answer, retrieve the top-k vectors, and feed those vectors into the generator to achieve a perplexity score for the correct answer.  Then minimize the KL-divergence between the observed retrieved vectors probability and LM likelihoods to adjust the retriever.[10] \\n use reranking to train the retriever.[11] \\n\\nLanguage model[edit]\\n\\nRetro language model for RAG.  Each Retro block consist of Attention, Chunked Cross Attention, and Feed Forward layers.  Black lettered boxes show data being changed, and blue lettering show the algorithm performing the changes.\\nBy redesigning the language model with the retriever in mind, a 25-times smaller network can get comparable perplexity as its much larger counterparts.[12]  Because it is trained from scratch, this method (Retro) incurs the heavy cost of training runs that the original RAG scheme avoided.  The hypothesis is that by giving domain knowledge during training, Retro needs less focus on domain and can devote its smaller weight resources only on language semantics.  The redesigned language model is shown here.  \\nIt has been reported that Retro is not reproducible , so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.[13]\\n\\nChunking[edit]\\nConverting domain data into vectors should be done thoughtfully.  It is naive to convert an entire document into a single vector and expect the retriever to find details in that document in response to a query.  There are various strategies on how to break up the data.  This is called Chunking.\\n\\n\\nDifferent data styles have patterns that correct chunking can take advantage of.\\nThree types of chunking strategies are:\\n\\n\\n Fixed length with overlap.  This is fast and easy.  Overlapping consecutive chunks help to maintain semantic context across chunks.\\n Syntax based chunks can break document up by sentences.  Libraries such as spaCy or NLTK can also help.\\n File format based chunking.  Certain file types have natural chunks built in and it\\'s best to respect them.  For example, code files are best chunked and vectorized as whole functions or classes.  HTML files should leave <table> or base64 encoded <img> elements intact.  Similar considerations should be taken for pdf files.  Libraries such as Unstructured or Langchain can assist with this method.\\n\\nChallenges[edit]\\nIf the external data source is large, retrieval can be slow. The use of RAG does not completely eliminate the general challenges faced by LLMs, including hallucination.[3]\\n\\nReferences[edit]\\n\\n\\n^ a b c d e f Gao, Yunfan; Xiong, Yun; Gao, Xinyu; Jia, Kangxiang; Pan, Jinliu; Bi, Yuxi; Dai, Yi; Sun, Jiawei; Wang, Meng; Wang, Haofen (2023). \"Retrieval-Augmented Generation for Large Language Models: A Survey\". arXiv:2312.10997 [cs.CL].\\n\\n^ a b \"What is RAG? - Retrieval-Augmented Generation AI Explained - AWS\". Amazon Web Services, Inc. Retrieved 16 July 2024.\\n\\n^ a b \"Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook\". freeCodeCamp.org. 11 June 2024. Retrieved 16 July 2024.\\n\\n^ Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\". Advances in Neural Information Processing Systems. 33. Curran Associates, Inc.: 9459–9474. arXiv:2005.11401.\\n\\n^  \"faiss\". GitHub. \\n\\n^ Khattab, Omar; Zaharia, Matei (2020). \"\"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\"\". \\n\\n^ Formal, Thibault; Lassance, Carlos; Piwowarski, Benjamin; Clinchant, Stéphane (2021). \"\"SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval\"\". \\n\\n^  Lee, Kenton; Chang, Ming-Wei; Toutanova, Kristina (2019). \"\"Latent Retrieval for Weakly Supervised Open Domain Question Answering\"\" (PDF). \\n\\n^ Lin, Sheng-Chieh; Asai, Akari (2023). \"\"How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval\"\" (PDF). \\n\\n^ Shi, Weijia; Min, Sewon (2023). \"\"REPLUG: Retrieval-Augmented Black-Box Language Models\"\". \\n\\n^ Ram, Ori; Levine, Yoav; Dalmedigos, Itay; Muhlgay, Dor; Shashua, Amnon; Leyton-Brown, Kevin; Shoham, Yoav (2023). \"\"In-Context Retrieval-Augmented Language Models\"\". \\n\\n^ Borgeaud, Sebastian; Mensch, Arthur (2021). \"\"Improving language models by retrieving from trillions of tokens\"\" (PDF). \\n\\n^ Wang, Boxin; Ping, Wei (2023). \"\"Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study\"\" (PDF). \\n\\n\\nvteGenerative AIConcepts\\nAutoencoder\\nDeep learning\\nGenerative adversarial network\\nGenerative pre-trained transformer\\nLarge language model\\nNeural network\\nPrompt engineering\\nRAG\\nRLHF\\nSelf-supervised learning\\nTransformer\\nVariational autoencoder\\nVision transformer\\nWord embedding\\nModelsText\\nClaude\\nGemini\\nGPT-2\\nGPT-3\\nGPT-4\\nLLaMA\\nImages\\nDALL-E\\nMidjourney\\nStable Diffusion\\nVideos\\nSora\\nMusics\\nSuno AI\\nUdio\\nCompanies\\nAnthropic\\nGoogle DeepMind\\nHugging Face\\nOpenAI\\nMeta AI\\nMistral AI\\n Category\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Retrieval-augmented_generation&oldid=1244241321\"\\nCategories: Large language modelsNatural language processingInformation retrieval systemsHidden categories: Articles with short descriptionShort description is different from WikidataArticles containing potentially dated statements from 2023All articles containing potentially dated statementsWikipedia articles needing clarification from August 2024\\n\\n\\n\\n\\n\\n\\n This page was last edited on 5 September 2024, at 21:56\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\"\n",
    "\n",
    "pages = WebBaseLoader(URL).load()\n",
    "\n",
    "context = pages[0].page_content\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "RAG stands for Retrieval-Augmented Generation. It is a type of generative artificial intelligence that combines the capabilities of information retrieval and language generation. RAG uses a large language model (LLM) to generate responses to user queries, but instead of relying solely on its own training data, it also incorporates relevant information from a specified set of documents or databases. This allows the model to provide more accurate and up-to-date responses to user queries.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Based on the following information, what is RAG? Context: \\n\\n\" + context\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many [other loaders available](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) we can leverage to obtain context information from a variety of sources. \n",
    "\n",
    "To do one more example, let's try to summarize a YouTube video based on its transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=pqWUuYTcG-o\"\n",
    "\n",
    "transcripts = YoutubeLoader.from_youtube_url(VIDEO_URL, add_video_info=True).load()\n",
    "\n",
    "context = transcripts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The video transcript appears to be a commencement speech delivered by Roger Federer, a renowned tennis player, at Dartmouth College's Class of 2024 graduation ceremony. Federer starts by expressing his excitement and gratitude for being awarded an honorary degree by the college.\n",
      "\n",
      "He then shares his personal story, mentioning that he left school at the age of 16 to pursue a full-time tennis career. Federer talks about how he had to work hard to achieve his goals and how he overcame obstacles, such as the perception that he played effortlessly. He emphasizes the importance of discipline, grit, and patience in achieving success.\n",
      "\n",
      "Federer shares three key lessons he has learned from his tennis career, which he believes can be applied to life beyond the court:\n",
      "\n",
      "1. Effortless is a myth: Federer explains that while people often perceive him as playing effortlessly, he had to work extremely hard to achieve his success. He encourages the graduates to recognize that success is not achieved overnight and that hard work is necessary.\n",
      "2. It's only a point: Federer shares a story about a match he lost to Rafael Nadal at Wimbledon in 2008. He explains that in tennis, even top-ranked players lose points, and it's essential to focus on the current point rather than dwelling on past mistakes.\n",
      "3. Life is bigger than the court: Federer emphasizes that while tennis has given him many memories and experiences, his off-court experiences, such as traveling, philanthropy, and relationships, are equally important. He encourages the graduates to explore the world, make a difference, and build strong relationships.\n",
      "\n",
      "Throughout the speech, Federer uses analogies from tennis to illustrate his points, making the speech relatable and engaging. He concludes by thanking Dartmouth for the honorary degree and encouraging the graduates to pursue their passions, be kind to one another, and have fun.\n",
      "\n",
      "Overall, Federer's speech is a motivational and inspiring message to the Class of 2024, reminding them that success is within their reach if they are willing to put in the effort and persevere through challenges.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize the following video transcript. Transcript: \\n\\n\" + context\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the size of the knowledge base increases, eventually the LLM will not be able to process all of this information at once. This is where the conventional RAG paradigm comes in: Instead of stuffing the entire context into the prompt, the knowledge base is indexed and queried based on the user's question to only retrieve relevant chunks of context. This sort of architecture is beyond the scope of this workshop, but check out the [`langchain_dartmouth` Cookbook](https://dartmouth-libraries.github.io/langchain-dartmouth-cookbook/) to explore more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
