{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple models\n",
    "\n",
    "`langchain_dartmouth` is part of the larger LangChain ecosystem, which includes packages for all major LLM providers, among many other things. We can therefore easily switch between different models by simply changing the class we are using for the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "gpt = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "llama = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gpt.invoke(\"Who are you?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama.invoke(\"Who are you?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even have the two language models have a conversation by passing their responses back and forth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"We are in an AI rap battle, with one AI being ketchup and the other one mustard.\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Who is the better condiment? Ketchup or mustard? Let's find out! Ketchup, kick us off!\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = gpt.invoke(conversation)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.append(response)\n",
    "response = llama.invoke(conversation)\n",
    "response.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
